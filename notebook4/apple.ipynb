{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30a0fabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\INDIA\\AppData\\Local\\Temp\\ipykernel_55200\\1235929226.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(ticker, period=period, interval=interval)\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance (Cross-Validation):\n",
      "                        MAE        R²\n",
      "Lasso Regression   0.784780  0.843676\n",
      "Linear Regression  0.785451  0.843396\n",
      "Ridge Regression   0.797764  0.840943\n",
      "Stacking           0.811515  0.836623\n",
      "Voting             2.708677 -0.791680\n",
      "Weighted Voting    3.131994 -1.481650\n",
      "Random Forest      3.722165 -2.633557\n",
      "Gradient Boosting  3.831217 -2.861564\n",
      "\n",
      "✅ Best ML Model Selected: Lasso Regression\n",
      "💾 Model saved as AAPL_hour.joblib\n",
      "\n",
      "📈 Predicted next close (ML AutoML) for AAPL (next hour): 224.22\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    VotingRegressor,\n",
    "    StackingRegressor\n",
    ")\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Download Intraday Stock Data (1h interval)\n",
    "# -------------------------------\n",
    "ticker = \"AAPL\"\n",
    "period = \"60d\"   # Yahoo Finance allows up to ~60 days for 1h interval\n",
    "interval = \"1h\"\n",
    "\n",
    "df = yf.download(ticker, period=period, interval=interval)\n",
    "\n",
    "# Flatten possible MultiIndex columns\n",
    "df.columns = df.columns.get_level_values(0)\n",
    "\n",
    "# Keep OHLC\n",
    "df = df.loc[:, [\"Open\", \"High\", \"Low\", \"Close\"]].reset_index()\n",
    "\n",
    "# Create target: Next hour's Close\n",
    "df[\"Next_Hour_Close\"] = df[\"Close\"].shift(-1)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Features & Preprocessor\n",
    "# -------------------------------\n",
    "features = [\"Open\", \"High\", \"Low\", \"Close\"]\n",
    "X = df[features]\n",
    "y = df[\"Next_Hour_Close\"]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"num\", StandardScaler(), features)],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Candidate ML Models\n",
    "# -------------------------------\n",
    "lr = LinearRegression()\n",
    "ridge = Ridge(alpha=1.0)\n",
    "lasso = Lasso(alpha=0.001, max_iter=30000)\n",
    "rf = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42)\n",
    "gb = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth=5, random_state=42)\n",
    "\n",
    "voting_reg = VotingRegressor(estimators=[(\"lr\", lr), (\"rf\", rf), (\"gb\", gb)])\n",
    "weighted_voting_reg = VotingRegressor(\n",
    "    estimators=[(\"lr\", lr), (\"rf\", rf), (\"gb\", gb)],\n",
    "    weights=[1, 2, 2]\n",
    ")\n",
    "stacking_reg = StackingRegressor(\n",
    "    estimators=[(\"lr\", lr), (\"rf\", rf), (\"gb\", gb)],\n",
    "    final_estimator=Ridge(alpha=1.0)\n",
    ")\n",
    "\n",
    "models = {\n",
    "    \"Linear Regression\": lr,\n",
    "    \"Ridge Regression\": ridge,\n",
    "    \"Lasso Regression\": lasso,\n",
    "    \"Random Forest\": rf,\n",
    "    \"Gradient Boosting\": gb,\n",
    "    \"Voting\": voting_reg,\n",
    "    \"Weighted Voting\": weighted_voting_reg,\n",
    "    \"Stacking\": stacking_reg\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Evaluate ML Models\n",
    "# -------------------------------\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    mae_scores, r2_scores = [], []\n",
    "    pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"model\", model)])\n",
    "    \n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        \n",
    "        mae_scores.append(mean_absolute_error(y_test, y_pred))\n",
    "        r2_scores.append(r2_score(y_test, y_pred))\n",
    "    \n",
    "    results[name] = {\"MAE\": np.mean(mae_scores), \"R²\": np.mean(r2_scores)}\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Results\n",
    "# -------------------------------\n",
    "results_df = pd.DataFrame(results).T.sort_values(by=\"MAE\", na_position=\"last\")\n",
    "print(\"\\nModel Performance (Cross-Validation):\")\n",
    "print(results_df)\n",
    "\n",
    "# -------------------------------\n",
    "# 6. AutoML: Pick Best ML Model\n",
    "# -------------------------------\n",
    "best_model_name = results_df.dropna().index[0]\n",
    "print(f\"\\n✅ Best ML Model Selected: {best_model_name}\")\n",
    "\n",
    "best_model = models[best_model_name]\n",
    "final_pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"model\", best_model)])\n",
    "final_pipeline.fit(X, y)\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Save Model as {ticker}_hour.joblib\n",
    "# -------------------------------\n",
    "model_filename = f\"{ticker}_hour.joblib\"\n",
    "joblib.dump(final_pipeline, model_filename)\n",
    "print(f\"💾 Model saved as {model_filename}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Predict Next Hour’s Price\n",
    "# -------------------------------\n",
    "latest_features = df[features].iloc[-1:]\n",
    "next_hour_prediction = final_pipeline.predict(latest_features)[0]\n",
    "\n",
    "print(f\"\\n📈 Predicted next close (ML AutoML) for {ticker} (next hour): {next_hour_prediction:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\INDIA\\AppData\\Local\\Temp\\ipykernel_55200\\411270449.py:23: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(ticker, start=start.strftime('%Y-%m-%d'),\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Selected columns, ['Open', 'High', 'Low', 'Close'], are not unique in dataframe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m X_train, X_test \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39miloc[train_idx], X\u001b[38;5;241m.\u001b[39miloc[test_idx]\n\u001b[0;32m     76\u001b[0m y_train, y_test \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39miloc[train_idx], y\u001b[38;5;241m.\u001b[39miloc[test_idx]\n\u001b[1;32m---> 77\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     79\u001b[0m mae_scores\u001b[38;5;241m.\u001b[39mappend(mean_absolute_error(y_test, y_pred))\n",
      "File \u001b[1;32md:\\new_stock_prediction\\venv\\lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\new_stock_prediction\\venv\\lib\\site-packages\\sklearn\\pipeline.py:655\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    649\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `transform_input` parameter can only be set if metadata \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    650\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouting is enabled. You can enable metadata routing using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    651\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`sklearn.set_config(enable_metadata_routing=True)`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    652\u001b[0m     )\n\u001b[0;32m    654\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m--> 655\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    657\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\new_stock_prediction\\venv\\lib\\site-packages\\sklearn\\pipeline.py:589\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, routed_params, raw_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m    583\u001b[0m step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    584\u001b[0m     step_idx\u001b[38;5;241m=\u001b[39mstep_idx,\n\u001b[0;32m    585\u001b[0m     step_params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[0;32m    586\u001b[0m     all_params\u001b[38;5;241m=\u001b[39mraw_params,\n\u001b[0;32m    587\u001b[0m )\n\u001b[1;32m--> 589\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32md:\\new_stock_prediction\\venv\\lib\\site-packages\\joblib\\memory.py:326\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\new_stock_prediction\\venv\\lib\\site-packages\\sklearn\\pipeline.py:1540\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m   1539\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1540\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\n\u001b[0;32m   1541\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1542\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   1543\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m   1544\u001b[0m         )\n",
      "File \u001b[1;32md:\\new_stock_prediction\\venv\\lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32md:\\new_stock_prediction\\venv\\lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\new_stock_prediction\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:988\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_transformers()\n\u001b[0;32m    986\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[1;32m--> 988\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_column_callables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[0;32m    991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n",
      "File \u001b[1;32md:\\new_stock_prediction\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:541\u001b[0m, in \u001b[0;36mColumnTransformer._validate_column_callables\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    539\u001b[0m         columns \u001b[38;5;241m=\u001b[39m columns(X)\n\u001b[0;32m    540\u001b[0m     all_columns\u001b[38;5;241m.\u001b[39mappend(columns)\n\u001b[1;32m--> 541\u001b[0m     transformer_to_input_indices[name] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_column_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_columns \u001b[38;5;241m=\u001b[39m all_columns\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer_to_input_indices \u001b[38;5;241m=\u001b[39m transformer_to_input_indices\n",
      "File \u001b[1;32md:\\new_stock_prediction\\venv\\lib\\site-packages\\sklearn\\utils\\_indexing.py:445\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    443\u001b[0m         col_idx \u001b[38;5;241m=\u001b[39m all_columns\u001b[38;5;241m.\u001b[39mget_loc(col)\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col_idx, numbers\u001b[38;5;241m.\u001b[39mIntegral):\n\u001b[1;32m--> 445\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    446\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelected columns, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, are not unique in dataframe\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    447\u001b[0m             )\n\u001b[0;32m    448\u001b[0m         column_indices\u001b[38;5;241m.\u001b[39mappend(col_idx)\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mValueError\u001b[0m: Selected columns, ['Open', 'High', 'Low', 'Close'], are not unique in dataframe"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    VotingRegressor,\n",
    "    StackingRegressor\n",
    ")\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 1. Download hourly stock data (last 90 days)\n",
    "ticker = \"AAPL\"\n",
    "end = datetime.today()\n",
    "start = end - timedelta(days=90)\n",
    "df = yf.download(ticker, start=start.strftime('%Y-%m-%d'),\n",
    "                 end=end.strftime('%Y-%m-%d'), interval=\"60m\")\n",
    "# Reset index to get a datetime column (for clarity)\n",
    "df = df.reset_index()\n",
    "\n",
    "# Use only OHLC columns\n",
    "df = df[[\"Datetime\",\"Open\",\"High\",\"Low\",\"Close\"]]\n",
    "\n",
    "# 2. Create target = next hour's Close\n",
    "df[\"Next_Close\"] = df[\"Close\"].shift(-1)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# 3. Prepare features and target\n",
    "features = [\"Open\", \"High\", \"Low\", \"Close\"]\n",
    "X = df[features]\n",
    "y = df[\"Next_Close\"]\n",
    "\n",
    "# Standardize the numeric features\n",
    "preprocessor = ColumnTransformer(transformers=[(\"num\", StandardScaler(), features)],\n",
    "                                 remainder=\"drop\")\n",
    "\n",
    "# 4. Define candidate models\n",
    "lr    = LinearRegression()\n",
    "ridge = Ridge(alpha=1.0)\n",
    "lasso = Lasso(alpha=0.001, max_iter=30000)\n",
    "rf    = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42)\n",
    "gb    = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth=5, random_state=42)\n",
    "voting_reg = VotingRegressor(estimators=[(\"lr\", lr), (\"rf\", rf), (\"gb\", gb)])\n",
    "weighted_voting_reg = VotingRegressor(estimators=[(\"lr\", lr), (\"rf\", rf), (\"gb\", gb)],\n",
    "                                     weights=[1,2,2])\n",
    "stacking_reg = StackingRegressor(\n",
    "    estimators=[(\"lr\", lr), (\"rf\", rf), (\"gb\", gb)],\n",
    "    final_estimator=Ridge(alpha=1.0)\n",
    ")\n",
    "models = {\n",
    "    \"Linear Regression\": lr,\n",
    "    \"Ridge Regression\": ridge,\n",
    "    \"Lasso Regression\": lasso,\n",
    "    \"Random Forest\": rf,\n",
    "    \"Gradient Boosting\": gb,\n",
    "    \"Voting\": voting_reg,\n",
    "    \"Weighted Voting\": weighted_voting_reg,\n",
    "    \"Stacking\": stacking_reg\n",
    "}\n",
    "\n",
    "# 5. Evaluate each model with time-series CV\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    mae_scores, r2_scores = [], []\n",
    "    pipeline = Pipeline(steps=[(\"scale\", preprocessor), (\"model\", model)])\n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        mae_scores.append(mean_absolute_error(y_test, y_pred))\n",
    "        r2_scores.append(r2_score(y_test, y_pred))\n",
    "    results[name] = {\"MAE\": np.mean(mae_scores), \"R²\": np.mean(r2_scores)}\n",
    "\n",
    "results_df = pd.DataFrame(results).T.sort_values(by=\"MAE\")\n",
    "print(\"Model CV performance (lower MAE is better):\")\n",
    "print(results_df)\n",
    "\n",
    "# 6. Select best model and retrain on all data\n",
    "best_model_name = results_df.index[0]\n",
    "print(f\"✅ Best model: {best_model_name}\")\n",
    "best_model = models[best_model_name]\n",
    "final_pipeline = Pipeline(steps=[(\"scale\", preprocessor), (\"model\", best_model)])\n",
    "final_pipeline.fit(X, y)\n",
    "\n",
    "# 7. Save the trained model pipeline\n",
    "model_filename = f\"{ticker}_hour.joblib\"\n",
    "joblib.dump(final_pipeline, model_filename)\n",
    "print(f\"💾 Saved model as {model_filename}\")\n",
    "\n",
    "# 8. Predict next hour's closing price\n",
    "latest_features = df[features].iloc[-1:].copy()\n",
    "pred_next_hour = final_pipeline.predict(latest_features)[0]\n",
    "print(f\"📈 Predicted next close (next hour) for {ticker}: {pred_next_hour:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e74078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\INDIA\\AppData\\Local\\Temp\\ipykernel_55200\\1235929226.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(ticker, period=period, interval=interval)\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance (Cross-Validation):\n",
      "                        MAE        R²\n",
      "Lasso Regression   0.784780  0.843676\n",
      "Linear Regression  0.785451  0.843396\n",
      "Ridge Regression   0.797764  0.840943\n",
      "Stacking           0.811515  0.836623\n",
      "Voting             2.708677 -0.791680\n",
      "Weighted Voting    3.131994 -1.481650\n",
      "Random Forest      3.722165 -2.633557\n",
      "Gradient Boosting  3.831217 -2.861564\n",
      "\n",
      "✅ Best ML Model Selected: Lasso Regression\n",
      "💾 Model saved as AAPL_hour.joblib\n",
      "\n",
      "📈 Predicted next close (ML AutoML) for AAPL (next hour): 224.22\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    VotingRegressor,\n",
    "    StackingRegressor\n",
    ")\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Download Intraday Stock Data (1h interval)\n",
    "# -------------------------------\n",
    "ticker = \"AAPL\"\n",
    "period = \"60d\"   # Yahoo Finance allows up to ~60 days for 1h interval\n",
    "interval = \"1h\"\n",
    "\n",
    "df = yf.download(ticker, period=period, interval=interval)\n",
    "\n",
    "# Flatten possible MultiIndex columns\n",
    "df.columns = df.columns.get_level_values(0)\n",
    "\n",
    "# Keep OHLC\n",
    "df = df.loc[:, [\"Open\", \"High\", \"Low\", \"Close\"]].reset_index()\n",
    "\n",
    "# Create target: Next hour's Close\n",
    "df[\"Next_Hour_Close\"] = df[\"Close\"].shift(-1)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Features & Preprocessor\n",
    "# -------------------------------\n",
    "features = [\"Open\", \"High\", \"Low\", \"Close\"]\n",
    "X = df[features]\n",
    "y = df[\"Next_Hour_Close\"]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"num\", StandardScaler(), features)],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Candidate ML Models\n",
    "# -------------------------------\n",
    "lr = LinearRegression()\n",
    "ridge = Ridge(alpha=1.0)\n",
    "lasso = Lasso(alpha=0.001, max_iter=30000)\n",
    "rf = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42)\n",
    "gb = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth=5, random_state=42)\n",
    "\n",
    "voting_reg = VotingRegressor(estimators=[(\"lr\", lr), (\"rf\", rf), (\"gb\", gb)])\n",
    "weighted_voting_reg = VotingRegressor(\n",
    "    estimators=[(\"lr\", lr), (\"rf\", rf), (\"gb\", gb)],\n",
    "    weights=[1, 2, 2]\n",
    ")\n",
    "stacking_reg = StackingRegressor(\n",
    "    estimators=[(\"lr\", lr), (\"rf\", rf), (\"gb\", gb)],\n",
    "    final_estimator=Ridge(alpha=1.0)\n",
    ")\n",
    "\n",
    "models = {\n",
    "    \"Linear Regression\": lr,\n",
    "    \"Ridge Regression\": ridge,\n",
    "    \"Lasso Regression\": lasso,\n",
    "    \"Random Forest\": rf,\n",
    "    \"Gradient Boosting\": gb,\n",
    "    \"Voting\": voting_reg,\n",
    "    \"Weighted Voting\": weighted_voting_reg,\n",
    "    \"Stacking\": stacking_reg\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Evaluate ML Models\n",
    "# -------------------------------\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    mae_scores, r2_scores = [], []\n",
    "    pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"model\", model)])\n",
    "    \n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        \n",
    "        mae_scores.append(mean_absolute_error(y_test, y_pred))\n",
    "        r2_scores.append(r2_score(y_test, y_pred))\n",
    "    \n",
    "    results[name] = {\"MAE\": np.mean(mae_scores), \"R²\": np.mean(r2_scores)}\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Results\n",
    "# -------------------------------\n",
    "results_df = pd.DataFrame(results).T.sort_values(by=\"MAE\", na_position=\"last\")\n",
    "print(\"\\nModel Performance (Cross-Validation):\")\n",
    "print(results_df)\n",
    "\n",
    "# -------------------------------\n",
    "# 6. AutoML: Pick Best ML Model\n",
    "# -------------------------------\n",
    "best_model_name = results_df.dropna().index[0]\n",
    "print(f\"\\n✅ Best ML Model Selected: {best_model_name}\")\n",
    "\n",
    "best_model = models[best_model_name]\n",
    "final_pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"model\", best_model)])\n",
    "final_pipeline.fit(X, y)\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Save Model as {ticker}_hour.joblib\n",
    "# -------------------------------\n",
    "model_filename = f\"{ticker}_hour.joblib\"\n",
    "joblib.dump(final_pipeline, model_filename)\n",
    "print(f\"💾 Model saved as {model_filename}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Predict Next Hour’s Price\n",
    "# -------------------------------\n",
    "latest_features = df[features].iloc[-1:]\n",
    "next_hour_prediction = final_pipeline.predict(latest_features)[0]\n",
    "\n",
    "print(f\"\\n📈 Predicted next close (ML AutoML) for {ticker} (next hour): {next_hour_prediction:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b099abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\INDIA\\AppData\\Local\\Temp\\ipykernel_55200\\1235929226.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(ticker, period=period, interval=interval)\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance (Cross-Validation):\n",
      "                        MAE        R²\n",
      "Lasso Regression   0.784780  0.843676\n",
      "Linear Regression  0.785451  0.843396\n",
      "Ridge Regression   0.797764  0.840943\n",
      "Stacking           0.811515  0.836623\n",
      "Voting             2.708677 -0.791680\n",
      "Weighted Voting    3.131994 -1.481650\n",
      "Random Forest      3.722165 -2.633557\n",
      "Gradient Boosting  3.831217 -2.861564\n",
      "\n",
      "✅ Best ML Model Selected: Lasso Regression\n",
      "💾 Model saved as AAPL_hour.joblib\n",
      "\n",
      "📈 Predicted next close (ML AutoML) for AAPL (next hour): 224.22\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    VotingRegressor,\n",
    "    StackingRegressor\n",
    ")\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Download Intraday Stock Data (1h interval)\n",
    "# -------------------------------\n",
    "ticker = \"AAPL\"\n",
    "period = \"60d\"   # Yahoo Finance allows up to ~60 days for 1h interval\n",
    "interval = \"1h\"\n",
    "\n",
    "df = yf.download(ticker, period=period, interval=interval)\n",
    "\n",
    "# Flatten possible MultiIndex columns\n",
    "df.columns = df.columns.get_level_values(0)\n",
    "\n",
    "# Keep OHLC\n",
    "df = df.loc[:, [\"Open\", \"High\", \"Low\", \"Close\"]].reset_index()\n",
    "\n",
    "# Create target: Next hour's Close\n",
    "df[\"Next_Hour_Close\"] = df[\"Close\"].shift(-1)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Features & Preprocessor\n",
    "# -------------------------------\n",
    "features = [\"Open\", \"High\", \"Low\", \"Close\"]\n",
    "X = df[features]\n",
    "y = df[\"Next_Hour_Close\"]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"num\", StandardScaler(), features)],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Candidate ML Models\n",
    "# -------------------------------\n",
    "lr = LinearRegression()\n",
    "ridge = Ridge(alpha=1.0)\n",
    "lasso = Lasso(alpha=0.001, max_iter=30000)\n",
    "rf = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42)\n",
    "gb = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth=5, random_state=42)\n",
    "\n",
    "voting_reg = VotingRegressor(estimators=[(\"lr\", lr), (\"rf\", rf), (\"gb\", gb)])\n",
    "weighted_voting_reg = VotingRegressor(\n",
    "    estimators=[(\"lr\", lr), (\"rf\", rf), (\"gb\", gb)],\n",
    "    weights=[1, 2, 2]\n",
    ")\n",
    "stacking_reg = StackingRegressor(\n",
    "    estimators=[(\"lr\", lr), (\"rf\", rf), (\"gb\", gb)],\n",
    "    final_estimator=Ridge(alpha=1.0)\n",
    ")\n",
    "\n",
    "models = {\n",
    "    \"Linear Regression\": lr,\n",
    "    \"Ridge Regression\": ridge,\n",
    "    \"Lasso Regression\": lasso,\n",
    "    \"Random Forest\": rf,\n",
    "    \"Gradient Boosting\": gb,\n",
    "    \"Voting\": voting_reg,\n",
    "    \"Weighted Voting\": weighted_voting_reg,\n",
    "    \"Stacking\": stacking_reg\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Evaluate ML Models\n",
    "# -------------------------------\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    mae_scores, r2_scores = [], []\n",
    "    pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"model\", model)])\n",
    "    \n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        \n",
    "        mae_scores.append(mean_absolute_error(y_test, y_pred))\n",
    "        r2_scores.append(r2_score(y_test, y_pred))\n",
    "    \n",
    "    results[name] = {\"MAE\": np.mean(mae_scores), \"R²\": np.mean(r2_scores)}\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Results\n",
    "# -------------------------------\n",
    "results_df = pd.DataFrame(results).T.sort_values(by=\"MAE\", na_position=\"last\")\n",
    "print(\"\\nModel Performance (Cross-Validation):\")\n",
    "print(results_df)\n",
    "\n",
    "# -------------------------------\n",
    "# 6. AutoML: Pick Best ML Model\n",
    "# -------------------------------\n",
    "best_model_name = results_df.dropna().index[0]\n",
    "print(f\"\\n✅ Best ML Model Selected: {best_model_name}\")\n",
    "\n",
    "best_model = models[best_model_name]\n",
    "final_pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"model\", best_model)])\n",
    "final_pipeline.fit(X, y)\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Save Model as {ticker}_hour.joblib\n",
    "# -------------------------------\n",
    "model_filename = f\"{ticker}_hour.joblib\"\n",
    "joblib.dump(final_pipeline, model_filename)\n",
    "print(f\"💾 Model saved as {model_filename}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Predict Next Hour’s Price\n",
    "# -------------------------------\n",
    "latest_features = df[features].iloc[-1:]\n",
    "next_hour_prediction = final_pipeline.predict(latest_features)[0]\n",
    "\n",
    "print(f\"\\n📈 Predicted next close (ML AutoML) for {ticker} (next hour): {next_hour_prediction:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a76945e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading AAPL hourly data from 2025-05-24 to 2025-08-22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\INDIA\\AppData\\Local\\Temp\\ipykernel_55200\\1445741801.py:42: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(ticker, start=start.strftime('%Y-%m-%d'),\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (421, 6)\n",
      "Date range: 2025-05-27 13:30:00+00:00 to 2025-08-21 17:30:00+00:00\n",
      "\n",
      "Model CV performance (lower MAE is better):\n",
      "                        MAE        R²\n",
      "Lasso Regression   0.785319  0.850124\n",
      "Linear Regression  0.785620  0.849783\n",
      "Ridge Regression   0.797277  0.847964\n",
      "Stacking           0.849233  0.830833\n",
      "Voting             2.655390 -0.611164\n",
      "Weighted Voting    3.064468 -1.223109\n",
      "Random Forest      3.633185 -2.230160\n",
      "Gradient Boosting  3.746075 -2.460183\n",
      "\n",
      "✅ Best model: Lasso Regression\n",
      "💾 Saved model as AAPL_hour.joblib\n",
      "📈 Predicted next close (next hour) for AAPL: 224.22\n",
      "\n",
      "Summary:\n",
      "Model file: AAPL_hour.joblib\n",
      "Best model: Lasso Regression\n",
      "Next hour prediction: $224.22\n",
      "✅ Loaded model: AAPL_hour.joblib\n",
      "Downloading recent AAPL hourly data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\INDIA\\AppData\\Local\\Temp\\ipykernel_55200\\1445741801.py:186: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(ticker, start=start.strftime('%Y-%m-%d'),\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "📈 HOURLY PREDICTION FOR AAPL\n",
      "==================================================\n",
      "Current Time: 2025-08-21 18:30:00+00:00\n",
      "Next Hour:    2025-08-21 19:30:00+00:00\n",
      "Current Close: $224.76\n",
      "Predicted Close: $224.74\n",
      "Predicted Change: $-0.02 (-0.01%) 📉\n",
      "Model: AAPL_hour.joblib\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\INDIA\\AppData\\Local\\Temp\\ipykernel_55200\\1445741801.py:280: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(ticker, start=start.strftime('%Y-%m-%d'),\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 317\u001b[0m\n\u001b[0;32m    314\u001b[0m     fig\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# Plot recent hourly data\u001b[39;00m\n\u001b[1;32m--> 317\u001b[0m \u001b[43mplot_hourly_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAAPL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdays_back\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 314\u001b[0m, in \u001b[0;36mplot_hourly_predictions\u001b[1;34m(ticker, days_back)\u001b[0m\n\u001b[0;32m    304\u001b[0m     fig\u001b[38;5;241m.\u001b[39madd_trace(go\u001b[38;5;241m.\u001b[39mBar(x\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDatetime\u001b[39m\u001b[38;5;124m'\u001b[39m], y\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVolume\u001b[39m\u001b[38;5;124m'\u001b[39m], name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVolume\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m    305\u001b[0m                    row\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    307\u001b[0m fig\u001b[38;5;241m.\u001b[39mupdate_layout(\n\u001b[0;32m    308\u001b[0m     title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Hourly Data (Last \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdays_back\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Days)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    309\u001b[0m     xaxis_title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    310\u001b[0m     yaxis_title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrice\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    311\u001b[0m     height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m\n\u001b[0;32m    312\u001b[0m )\n\u001b[1;32m--> 314\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\new_stock_prediction\\venv\\lib\\site-packages\\plotly\\basedatatypes.py:3420\u001b[0m, in \u001b[0;36mBaseFigure.show\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3387\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3388\u001b[0m \u001b[38;5;124;03mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[0;32m   3389\u001b[0m \u001b[38;5;124;03mspecified by the renderer argument\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3416\u001b[0m \u001b[38;5;124;03mNone\u001b[39;00m\n\u001b[0;32m   3417\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3418\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpio\u001b[39;00m\n\u001b[1;32m-> 3420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pio\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\new_stock_prediction\\venv\\lib\\site-packages\\plotly\\io\\_renderers.py:415\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m     )\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nbformat \u001b[38;5;129;01mor\u001b[39;00m Version(nbformat\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    416\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m     )\n\u001b[0;32m    419\u001b[0m display_jupyter_version_warnings()\n\u001b[0;32m    421\u001b[0m ipython_display\u001b[38;5;241m.\u001b[39mdisplay(bundle, raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FIXED CODE FOR NOTEBOOK - Copy this into your notebook cells\n",
    "# ============================================================================\n",
    "\n",
    "# CELL 1: Training Function (Copy this into your first cell)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    VotingRegressor,\n",
    "    StackingRegressor\n",
    ")\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def train_hourly_model(ticker=\"AAPL\", days_back=90):\n",
    "    \"\"\"\n",
    "    Train an hourly prediction model for a given ticker.\n",
    "    \n",
    "    Args:\n",
    "        ticker (str): Stock ticker symbol\n",
    "        days_back (int): Number of days to look back for training data\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model_filename, best_model_name, prediction)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Download hourly stock data\n",
    "    end = datetime.today()\n",
    "    start = end - timedelta(days=days_back)\n",
    "    \n",
    "    print(f\"Downloading {ticker} hourly data from {start.strftime('%Y-%m-%d')} to {end.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    df = yf.download(ticker, start=start.strftime('%Y-%m-%d'),\n",
    "                     end=end.strftime('%Y-%m-%d'), interval=\"60m\")\n",
    "\n",
    "    # Properly handle MultiIndex columns\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        # If MultiIndex, get the first level (actual column names)\n",
    "        df.columns = df.columns.get_level_values(0)\n",
    "    \n",
    "    # Reset index to get a datetime column\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # Ensure we have the required columns and they are unique\n",
    "    required_cols = [\"Datetime\", \"Open\", \"High\", \"Low\", \"Close\"]\n",
    "    available_cols = df.columns.tolist()\n",
    "\n",
    "    # Check if all required columns exist\n",
    "    missing_cols = [col for col in required_cols if col not in available_cols]\n",
    "    if missing_cols:\n",
    "        print(f\"Missing columns: {missing_cols}\")\n",
    "        print(f\"Available columns: {available_cols}\")\n",
    "        raise ValueError(f\"Required columns {missing_cols} not found in data\")\n",
    "\n",
    "    # Use only OHLC columns\n",
    "    df = df[required_cols]\n",
    "\n",
    "    # 2. Create target = next hour's Close\n",
    "    df[\"Next_Close\"] = df[\"Close\"].shift(-1)\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "    print(f\"Training data shape: {df.shape}\")\n",
    "    print(f\"Date range: {df['Datetime'].min()} to {df['Datetime'].max()}\")\n",
    "\n",
    "    # 3. Prepare features and target\n",
    "    features = [\"Open\", \"High\", \"Low\", \"Close\"]\n",
    "    X = df[features]\n",
    "    y = df[\"Next_Close\"]\n",
    "\n",
    "    # Standardize the numeric features\n",
    "    preprocessor = ColumnTransformer(transformers=[(\"num\", StandardScaler(), features)],\n",
    "                                     remainder=\"drop\")\n",
    "\n",
    "    # 4. Define candidate models\n",
    "    lr    = LinearRegression()\n",
    "    ridge = Ridge(alpha=1.0)\n",
    "    lasso = Lasso(alpha=0.001, max_iter=30000)\n",
    "    rf    = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42)\n",
    "    gb    = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth=5, random_state=42)\n",
    "    voting_reg = VotingRegressor(estimators=[(\"lr\", lr), (\"rf\", rf), (\"gb\", gb)])\n",
    "    weighted_voting_reg = VotingRegressor(estimators=[(\"lr\", lr), (\"rf\", rf), (\"gb\", gb)],\n",
    "                                         weights=[1,2,2])\n",
    "    stacking_reg = StackingRegressor(\n",
    "        estimators=[(\"lr\", lr), (\"rf\", rf), (\"gb\", gb)],\n",
    "        final_estimator=Ridge(alpha=1.0)\n",
    "    )\n",
    "    models = {\n",
    "        \"Linear Regression\": lr,\n",
    "        \"Ridge Regression\": ridge,\n",
    "        \"Lasso Regression\": lasso,\n",
    "        \"Random Forest\": rf,\n",
    "        \"Gradient Boosting\": gb,\n",
    "        \"Voting\": voting_reg,\n",
    "        \"Weighted Voting\": weighted_voting_reg,\n",
    "        \"Stacking\": stacking_reg\n",
    "    }\n",
    "\n",
    "    # 5. Evaluate each model with time-series CV\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        mae_scores, r2_scores = [], []\n",
    "        pipeline = Pipeline(steps=[(\"scale\", preprocessor), (\"model\", model)])\n",
    "        for train_idx, test_idx in tscv.split(X):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "            mae_scores.append(mean_absolute_error(y_test, y_pred))\n",
    "            r2_scores.append(r2_score(y_test, y_pred))\n",
    "        results[name] = {\"MAE\": np.mean(mae_scores), \"R²\": np.mean(r2_scores)}\n",
    "\n",
    "    results_df = pd.DataFrame(results).T.sort_values(by=\"MAE\")\n",
    "    print(\"\\nModel CV performance (lower MAE is better):\")\n",
    "    print(results_df)\n",
    "\n",
    "    # 6. Select best model and retrain on all data\n",
    "    best_model_name = results_df.index[0]\n",
    "    print(f\"\\n✅ Best model: {best_model_name}\")\n",
    "    best_model = models[best_model_name]\n",
    "    final_pipeline = Pipeline(steps=[(\"scale\", preprocessor), (\"model\", best_model)])\n",
    "    final_pipeline.fit(X, y)\n",
    "\n",
    "    # 7. Save the trained model pipeline\n",
    "    model_filename = f\"{ticker}_hour.joblib\"\n",
    "    joblib.dump(final_pipeline, model_filename)\n",
    "    print(f\"💾 Saved model as {model_filename}\")\n",
    "\n",
    "    # 8. Predict next hour's closing price\n",
    "    latest_features = df[features].iloc[-1:].copy()\n",
    "    pred_next_hour = final_pipeline.predict(latest_features)[0]\n",
    "    print(f\"📈 Predicted next close (next hour) for {ticker}: {pred_next_hour:.2f}\")\n",
    "    \n",
    "    return model_filename, best_model_name, pred_next_hour\n",
    "\n",
    "# Train model for AAPL\n",
    "model_file, best_model, prediction = train_hourly_model(\"AAPL\", days_back=90)\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"Model file: {model_file}\")\n",
    "print(f\"Best model: {best_model}\")\n",
    "print(f\"Next hour prediction: ${prediction:.2f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 2: Prediction Function (Copy this into your second cell)\n",
    "# ============================================================================\n",
    "\n",
    "def predict_next_hour(ticker, model_filename=None):\n",
    "    \"\"\"\n",
    "    Predict the next hour's closing price using a saved model.\n",
    "    \n",
    "    Args:\n",
    "        ticker (str): Stock ticker symbol\n",
    "        model_filename (str): Path to the saved model file. If None, uses default naming.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Prediction results\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use default model filename if not provided\n",
    "    if model_filename is None:\n",
    "        model_filename = f\"{ticker}_hour.joblib\"\n",
    "    \n",
    "    try:\n",
    "        # Load the trained model\n",
    "        pipeline = joblib.load(model_filename)\n",
    "        print(f\"✅ Loaded model: {model_filename}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Model file {model_filename} not found. Please train the model first.\")\n",
    "        return None\n",
    "    \n",
    "    # Download recent hourly data for prediction\n",
    "    end = datetime.today()\n",
    "    start = end - timedelta(days=7)  # Get last 7 days of hourly data\n",
    "    \n",
    "    print(f\"Downloading recent {ticker} hourly data...\")\n",
    "    \n",
    "    df = yf.download(ticker, start=start.strftime('%Y-%m-%d'),\n",
    "                     end=end.strftime('%Y-%m-%d'), interval=\"60m\")\n",
    "    \n",
    "    # Handle MultiIndex columns\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = df.columns.get_level_values(0)\n",
    "    \n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # Ensure we have required columns\n",
    "    required_cols = [\"Datetime\", \"Open\", \"High\", \"Low\", \"Close\"]\n",
    "    available_cols = df.columns.tolist()\n",
    "    \n",
    "    missing_cols = [col for col in required_cols if col not in available_cols]\n",
    "    if missing_cols:\n",
    "        print(f\"❌ Missing columns: {missing_cols}\")\n",
    "        return None\n",
    "    \n",
    "    # Use only OHLC columns\n",
    "    df = df[required_cols]\n",
    "    \n",
    "    # Get the latest data point for prediction\n",
    "    latest_data = df.iloc[-1]\n",
    "    latest_features = df[[\"Open\", \"High\", \"Low\", \"Close\"]].iloc[-1:].copy()\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = pipeline.predict(latest_features)[0]\n",
    "    \n",
    "    # Calculate next hour timestamp\n",
    "    next_hour = latest_data[\"Datetime\"] + timedelta(hours=1)\n",
    "    \n",
    "    # Prepare results\n",
    "    results = {\n",
    "        \"ticker\": ticker,\n",
    "        \"current_datetime\": latest_data[\"Datetime\"],\n",
    "        \"next_hour_datetime\": next_hour,\n",
    "        \"current_close\": latest_data[\"Close\"],\n",
    "        \"predicted_next_close\": prediction,\n",
    "        \"predicted_change\": prediction - latest_data[\"Close\"],\n",
    "        \"predicted_change_percent\": ((prediction - latest_data[\"Close\"]) / latest_data[\"Close\"]) * 100,\n",
    "        \"model_file\": model_filename\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_prediction_results(results):\n",
    "    \"\"\"Pretty print the prediction results.\"\"\"\n",
    "    if results is None:\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"📈 HOURLY PREDICTION FOR {results['ticker']}\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Current Time: {results['current_datetime']}\")\n",
    "    print(f\"Next Hour:    {results['next_hour_datetime']}\")\n",
    "    print(f\"Current Close: ${results['current_close']:.2f}\")\n",
    "    print(f\"Predicted Close: ${results['predicted_next_close']:.2f}\")\n",
    "    \n",
    "    change = results['predicted_change']\n",
    "    change_pct = results['predicted_change_percent']\n",
    "    \n",
    "    if change > 0:\n",
    "        print(f\"Predicted Change: +${change:.2f} (+{change_pct:.2f}%) 📈\")\n",
    "    else:\n",
    "        print(f\"Predicted Change: ${change:.2f} ({change_pct:.2f}%) 📉\")\n",
    "    \n",
    "    print(f\"Model: {results['model_file']}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# Example usage - predict next hour for AAPL\n",
    "results = predict_next_hour(\"AAPL\")\n",
    "\n",
    "if results:\n",
    "    print_prediction_results(results)\n",
    "else:\n",
    "    print(f\"\\nTo train a model for AAPL, run the training cell above first.\")\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 3: Visualization (Copy this into your third cell)\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_hourly_predictions(ticker=\"AAPL\", days_back=7):\n",
    "    \"\"\"\n",
    "    Plot recent hourly data and predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Download recent data\n",
    "    end = datetime.today()\n",
    "    start = end - timedelta(days=days_back)\n",
    "    \n",
    "    df = yf.download(ticker, start=start.strftime('%Y-%m-%d'),\n",
    "                     end=end.strftime('%Y-%m-%d'), interval=\"60m\")\n",
    "    \n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = df.columns.get_level_values(0)\n",
    "    \n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # Create plot\n",
    "    fig = make_subplots(rows=2, cols=1, \n",
    "                        subplot_titles=(f'{ticker} Hourly OHLC', 'Volume'),\n",
    "                        vertical_spacing=0.1)\n",
    "    \n",
    "    # Candlestick chart\n",
    "    fig.add_trace(go.Candlestick(x=df['Datetime'],\n",
    "                                  open=df['Open'],\n",
    "                                  high=df['High'],\n",
    "                                  low=df['Low'],\n",
    "                                  close=df['Close'],\n",
    "                                  name='OHLC'),\n",
    "                   row=1, col=1)\n",
    "    \n",
    "    # Volume chart\n",
    "    if 'Volume' in df.columns:\n",
    "        fig.add_trace(go.Bar(x=df['Datetime'], y=df['Volume'], name='Volume'),\n",
    "                       row=2, col=1)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'{ticker} Hourly Data (Last {days_back} Days)',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Price',\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Plot recent hourly data\n",
    "plot_hourly_predictions(\"AAPL\", days_back=7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
